{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader.data_loader import PhageLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, output_labels, number_of_layers=1, bidirectional=True, weights_matrix=None):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim_dense = hidden_size\n",
    "        self.num_layers = number_of_layers \n",
    "        if bidirectional:\n",
    "            self.hidden_dim_dense = hidden_size * 2\n",
    "        if len(weights_matrix.size())!=0:\n",
    "            self.emb_layer = self.create_emb_layer(weights_matrix)          \n",
    "        else:\n",
    "            self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "            \n",
    "        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, bidirectional=bidirectional, num_layers=number_of_layers,batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim_dense, output_labels)\n",
    "        \n",
    "        \n",
    "    def forward(self, h_t1, indexes):\n",
    "        #indexes -> (batch,seq_length)\n",
    "        embedding = self.emb_layer(indexes)\n",
    "        #print(\"EMBEDDING SHAPE: \", embedding.size())\n",
    "        #embedding -> (batch,seq_length,embedding_size)\n",
    "        out,h_t = self.gru(embedding, h_t1)\n",
    "        #print(\"OUT SHAPE: \", out.size(), self.hidden_dim_dense)\n",
    "        #out -> ()\n",
    "        #out = out.view(self.hidden_dim_dense, -1)\n",
    "        \n",
    "        out = F.relu(self.linear(out))\n",
    "        out2 = F.softmax(out,dim=2)\n",
    "        out = F.log_softmax(out,dim=2)\n",
    "        \n",
    "        return out, out2,h_t\n",
    "        \n",
    "    def initHidden(self, batch_size, hidden_size):\n",
    "        if self.bidirectional:\n",
    "            return torch.randn(self.num_layers*2, batch_size, hidden_size, device=device)\n",
    "        else:\n",
    "            return torch.randn(self.num_layers, batch_size, hidden_size, device=device)\n",
    "\n",
    "    def create_emb_layer(self, weights_matrix, non_trainable=False):\n",
    "        num_embeddings, embedding_dim = weights_matrix.size()\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "        if non_trainable:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PhageLoader(\"data/\")\n",
    "read_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size=1\n",
    "batch_size = 16\n",
    "read_length = 100\n",
    "dataset = loader.get_data_set(n_files='all',read_length=read_length, batch_size=batch_size, k=k_size, stride=1, embedding=\"dict\", embed_size=None, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(dataset):\n",
    "    n = len(dataset)  # how many total elements you have\n",
    "    test_size = .1\n",
    "    n_test = int( n * test_size )  # number of test/val elements\n",
    "    n_train = n - 2 * n_test\n",
    "\n",
    "    idx = list(range(n))  # indices to all elements\n",
    "    np.random.shuffle(idx)  # in-place shuffle the indices to facilitate random splitting\n",
    "    train_idx = idx[:n_train]\n",
    "    val_idx = idx[n_train:(n_train + n_test)]\n",
    "    test_idx = idx[(n_train + n_test):]\n",
    "\n",
    "    print(n,len(train_idx),len(val_idx),len(test_idx))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(val_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                               sampler=train_sampler,drop_last=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                    sampler=valid_sampler,drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                    sampler=test_sampler,drop_last=True)\n",
    "    return train_loader, validation_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_embeddings(k=3):\n",
    "    if(k<=2):\n",
    "        return torch.from_numpy(np.eye(4**k))\n",
    "    dictionary = loader.get_dict(k,'dna2vec')\n",
    "    indexes = loader.get_dict(k)\n",
    "    size = len(indexes)\n",
    "    matrix = np.zeros((size,100))\n",
    "    for key, value in indexes.items():\n",
    "        matrix[value] = dictionary[key]\n",
    "    return torch.from_numpy(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(model):\n",
    "    gru = model.gru\n",
    "    for p,n in zip(gru.parameters(),gru._all_weights[0]):\n",
    "        if n[:6] == 'weight':\n",
    "            print('===========\\ngradient:{}\\n----------\\n{}'.format(n,p.grad.abs().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-5.5212e-01, -9.0010e-01, -2.4534e-01, -4.5929e-01,  7.2408e-01,\n",
      "          -2.7522e-01, -1.4250e+00, -5.1070e-01, -5.8889e-01, -5.9803e-01,\n",
      "          -3.4178e-01, -1.1529e+00,  1.5320e+00, -9.5519e-01, -5.2789e-01,\n",
      "           1.7838e+00],\n",
      "         [-3.7691e-01, -6.6096e-01, -6.4870e-01, -5.7640e-01, -2.6654e-01,\n",
      "          -5.7906e-01,  1.1511e+00, -8.2056e-01,  1.5204e+00,  1.6127e-01,\n",
      "           1.0147e+00,  1.6614e+00, -1.1325e+00, -9.0000e-01,  5.6217e-01,\n",
      "           4.1829e-01],\n",
      "         [ 9.9227e-02, -6.3150e-01, -1.1611e+00,  6.3215e-01, -3.3317e-01,\n",
      "           2.6595e-01,  5.0291e-01,  1.0916e+00, -8.6107e-01, -2.8782e-01,\n",
      "           9.0368e-01, -2.4702e-01,  3.8838e-01,  1.9176e-01,  2.7612e-01,\n",
      "           4.3105e-01],\n",
      "         [-1.6332e+00, -1.4157e+00, -4.3189e-01,  1.1062e+00,  1.1501e+00,\n",
      "          -9.4518e-01,  2.5337e-01,  1.9763e+00, -5.7808e-01,  2.4201e-01,\n",
      "          -8.6053e-01, -7.6178e-01, -1.5077e-01,  4.6349e-01, -6.6602e-01,\n",
      "           1.0706e-01],\n",
      "         [-4.1543e-01,  2.2797e-01, -1.5296e-02,  9.8138e-01, -5.9949e-01,\n",
      "          -2.8138e+00, -1.3429e+00,  1.3472e-02,  1.2312e+00,  6.5127e-01,\n",
      "          -9.7551e-01, -1.1852e-01, -6.5477e-02,  6.2721e-01, -4.5201e-01,\n",
      "           1.5463e+00],\n",
      "         [ 2.5013e-01, -8.5654e-01,  1.2602e+00,  9.3600e-01, -7.1104e-01,\n",
      "           1.8786e-01, -1.7904e+00, -4.1653e-01, -7.2644e-01,  3.3563e-01,\n",
      "          -5.6894e-01,  2.0188e+00, -2.1971e+00, -7.7896e-01, -4.5469e-01,\n",
      "          -1.0937e+00],\n",
      "         [ 1.1381e+00,  5.0866e-02, -1.3205e+00, -1.3182e+00, -1.0206e+00,\n",
      "          -2.8796e-01,  8.4659e-01,  1.1065e+00,  1.7633e+00, -1.8359e+00,\n",
      "           1.1448e+00, -3.2666e-01,  2.6211e+00,  3.8049e-01,  5.8684e-01,\n",
      "           4.9657e-01],\n",
      "         [-1.5922e+00,  3.9091e-01, -2.8998e-01,  3.6307e-01, -6.0042e-01,\n",
      "          -3.9848e-01, -1.0202e+00, -6.4673e-01,  1.5094e+00,  2.5225e-01,\n",
      "           3.2445e-01,  2.4325e-02, -1.1012e+00,  7.8753e-01,  1.0962e-01,\n",
      "          -1.1378e-01],\n",
      "         [-4.3426e-02,  5.5044e-01,  1.3506e+00, -3.4474e-01,  4.4110e-01,\n",
      "          -7.1641e-01, -7.5484e-01,  5.0206e-02, -3.1107e-01,  1.3301e+00,\n",
      "          -2.4910e-01,  2.7715e-01, -2.9904e-02, -3.4786e-02, -8.3133e-02,\n",
      "           1.0256e+00],\n",
      "         [-7.6253e-01, -3.3318e-01,  1.4798e-01, -1.5707e-01, -3.4703e-01,\n",
      "           6.4610e-01,  1.3567e+00, -1.3934e+00,  9.6165e-01,  3.6567e-01,\n",
      "           4.8135e-01,  1.2345e+00,  2.9173e+00,  2.0141e-01, -2.0150e+00,\n",
      "          -1.0956e+00],\n",
      "         [-1.2416e+00,  6.4089e-03,  3.9026e-01,  6.6277e-01,  2.1541e+00,\n",
      "          -1.2316e-01,  1.1649e-01,  1.2150e-01, -7.4084e-01, -2.8958e-01,\n",
      "           9.0247e-01,  3.3859e-01, -9.9233e-02,  2.4150e+00, -3.9401e-01,\n",
      "          -3.0363e-01],\n",
      "         [ 4.4763e-01,  2.2558e-01, -2.0496e+00,  7.5222e-01,  4.0306e-01,\n",
      "          -1.2395e+00,  1.8093e-01,  7.2210e-01, -3.0795e-01, -2.2996e+00,\n",
      "           4.6411e-01, -1.1075e+00, -4.2498e-01,  2.9935e-02, -2.7487e-01,\n",
      "           9.2121e-01],\n",
      "         [-1.7889e+00,  2.1939e-01,  1.1389e+00, -8.5141e-01,  1.0898e+00,\n",
      "           4.6374e-01,  1.4866e-01, -5.5068e-02, -3.3611e-01, -9.9775e-01,\n",
      "           4.9888e-01,  2.0987e-01,  5.0274e-01,  1.2281e+00, -1.7891e-01,\n",
      "           8.3006e-01],\n",
      "         [ 7.1660e-01, -3.7735e-01,  1.5868e-01,  1.4605e+00,  1.3111e-01,\n",
      "           3.3845e-01, -7.4668e-02, -5.4194e-01, -1.1284e-01, -1.1019e+00,\n",
      "           1.2355e+00, -1.8604e-01, -1.7940e+00, -3.8883e-01, -1.5459e+00,\n",
      "          -3.1168e-01],\n",
      "         [ 4.0837e-01, -1.3037e+00,  1.5454e+00, -3.3730e-01, -1.8383e-02,\n",
      "           3.4058e-01,  2.4343e+00, -5.1998e-01, -8.8616e-01, -3.3609e-01,\n",
      "          -1.2782e-01,  3.8206e-01,  6.5203e-01, -1.7718e+00, -2.4321e-02,\n",
      "          -6.3053e-02],\n",
      "         [-6.0599e-01,  3.9001e-01, -3.8223e-01,  7.2949e-01, -1.1537e+00,\n",
      "          -8.2492e-01,  1.5877e+00,  2.2439e-01, -6.2853e-01, -9.3455e-01,\n",
      "          -4.8719e-01,  5.6088e-01, -2.2100e+00,  3.3290e-01, -3.3265e-01,\n",
      "           3.5303e-01]],\n",
      "\n",
      "        [[-9.1718e-01,  1.8298e-01,  2.4570e-01,  1.6390e+00, -1.5772e+00,\n",
      "          -6.7137e-01, -4.1530e-01,  1.9720e-01,  7.1217e-01, -1.4509e+00,\n",
      "          -1.2609e-01,  2.2367e-01, -1.2156e+00, -1.2660e+00, -1.5567e-01,\n",
      "           1.1363e+00],\n",
      "         [-2.3597e+00, -2.5281e-01,  1.3578e+00, -2.7675e-04,  1.2474e+00,\n",
      "           2.0393e+00, -2.3960e-01, -1.7435e+00,  8.3241e-01,  1.0270e+00,\n",
      "          -8.8471e-01, -2.9767e+00, -7.2534e-01,  4.4612e-01, -1.8115e-01,\n",
      "           3.0476e-01],\n",
      "         [-1.8365e-01, -4.6121e-01, -5.2049e-02,  2.9826e-01,  5.2251e-01,\n",
      "          -4.9552e-01, -1.6501e+00,  1.1637e+00,  1.0824e-01,  1.1197e+00,\n",
      "           2.9750e+00, -5.5537e-01, -2.7167e-01, -4.1594e-01,  4.6550e-01,\n",
      "          -9.3496e-01],\n",
      "         [-5.7011e-04,  4.1963e-01,  2.0912e+00, -5.6281e-01, -1.1335e+00,\n",
      "          -8.8537e-01,  1.1396e+00, -2.3372e+00, -4.9569e-01, -1.9805e+00,\n",
      "           9.5607e-01,  8.3390e-01, -1.3067e-01, -1.3846e+00, -1.2399e-02,\n",
      "          -4.2696e-01],\n",
      "         [-8.5918e-01, -1.8428e+00, -6.9438e-01,  1.6110e+00, -8.5817e-01,\n",
      "          -1.0729e+00, -2.6215e-01, -1.3406e+00,  3.6092e-01, -1.1671e+00,\n",
      "          -8.3136e-01, -1.0823e+00,  7.3840e-01,  8.2844e-01, -1.8865e-01,\n",
      "           8.5870e-02],\n",
      "         [ 3.7092e-01,  2.8440e-01, -1.2900e-01, -1.4356e+00, -2.0982e-02,\n",
      "          -3.8135e-01, -4.5069e-01, -2.3770e-01, -1.8656e+00,  1.8789e+00,\n",
      "          -4.4070e-01,  4.6983e-01,  1.2671e-01,  1.3501e+00,  5.9163e-01,\n",
      "          -5.5977e-02],\n",
      "         [ 3.1173e-01, -4.4880e-01, -1.2907e+00, -8.2149e-01, -3.0715e-01,\n",
      "          -8.9382e-01, -8.5043e-01,  1.7836e+00,  3.6470e-01, -8.2257e-01,\n",
      "          -2.1017e-01, -1.1677e+00,  4.9826e-01, -4.7977e-01, -7.8634e-01,\n",
      "          -3.4397e-01],\n",
      "         [ 1.5513e+00,  1.0700e+00,  4.6107e-02,  6.4921e-01,  2.4505e-01,\n",
      "          -1.2612e+00, -1.9821e+00,  8.6737e-02, -3.8222e-01,  3.0172e-01,\n",
      "           2.0130e+00,  1.8977e+00, -4.1040e-01, -6.7824e-01, -1.9520e+00,\n",
      "          -2.4063e+00],\n",
      "         [ 3.6574e-01,  1.5602e+00,  3.2212e-01, -1.6404e+00,  3.4222e-01,\n",
      "          -3.0636e-01, -4.8274e-01,  1.6858e+00,  1.6076e+00,  1.9228e-01,\n",
      "           1.3321e+00,  7.2716e-01,  7.2536e-01,  8.3622e-01, -4.5604e-02,\n",
      "          -3.1156e-01],\n",
      "         [ 4.8445e-01,  3.0034e-01,  1.4983e+00,  5.3170e-01,  1.5314e+00,\n",
      "           2.8303e+00,  8.3197e-02, -7.6769e-01, -7.7645e-02,  1.4235e+00,\n",
      "           1.1420e-01, -1.7739e-01, -3.3679e-01, -1.3661e+00,  1.6750e+00,\n",
      "          -6.7852e-01],\n",
      "         [ 1.3088e+00, -6.4168e-01, -1.2725e+00,  8.3604e-01, -1.7280e+00,\n",
      "           8.3483e-01,  1.2599e-01,  5.3887e-01, -5.3929e-02,  1.2903e-01,\n",
      "           1.4861e-01,  1.4628e+00, -9.1546e-01, -2.5114e-01,  2.2105e+00,\n",
      "           1.4827e+00],\n",
      "         [-3.9955e-01,  5.5460e-01,  1.6613e-01, -1.0369e+00,  3.8103e-01,\n",
      "           1.7636e+00,  8.3706e-01,  1.0088e-01, -8.7768e-02, -6.4519e-01,\n",
      "           1.6498e+00,  1.6617e+00, -1.5163e+00,  8.3321e-01, -5.0372e-01,\n",
      "          -1.0084e+00],\n",
      "         [-1.1736e+00,  1.6215e+00, -5.8611e-01,  1.0924e+00,  8.2977e-01,\n",
      "           1.1597e+00, -1.4891e+00, -1.4177e-01,  2.9003e-01, -1.4863e+00,\n",
      "          -1.0009e+00,  1.6956e+00,  1.1566e-01,  3.6977e-01, -2.0597e-01,\n",
      "          -1.9110e+00],\n",
      "         [-4.2376e-01,  3.6432e-01,  4.3848e-01, -1.0635e+00,  2.2483e+00,\n",
      "           2.9455e-01,  2.0807e-01,  6.3943e-01, -1.1168e+00, -1.7614e-01,\n",
      "          -2.5218e+00,  9.1261e-01,  1.3769e+00,  5.7848e-01,  4.2554e-01,\n",
      "           6.4451e-01],\n",
      "         [-1.2084e+00, -1.0793e+00,  3.1799e-01, -7.0520e-01, -1.8523e-01,\n",
      "           2.3209e+00, -1.0085e+00,  3.9145e-01, -1.6655e+00,  6.0133e-01,\n",
      "           1.6131e+00, -1.2515e+00, -1.3243e+00,  1.5541e+00,  2.5780e+00,\n",
      "          -9.3676e-02],\n",
      "         [ 1.7334e+00,  3.8641e-01,  1.4364e-01,  7.4431e-01, -3.3162e-01,\n",
      "           1.5801e-01, -2.9419e-01,  6.6508e-01,  9.3311e-01,  8.7530e-01,\n",
      "           1.8995e+00, -4.8363e-02, -1.3032e-01, -9.7492e-01, -4.5810e-01,\n",
      "          -2.6266e-01]]])\n",
      "8319 6657 831 831\n",
      "[1,    20] loss: 0.114\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.38789427280426025\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.34257936477661133\n",
      "[1,    40] loss: 0.097\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.3502192795276642\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.3179705739021301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    60] loss: 0.079\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.3335218131542206\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.320604145526886\n",
      "[1,    80] loss: 0.080\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.13990512490272522\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.13841937482357025\n",
      "[1,   100] loss: 0.079\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.13370203971862793\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.13777995109558105\n",
      "[1,   120] loss: 0.075\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.044608019292354584\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.02331388369202614\n",
      "[1,   140] loss: 0.060\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.2683221697807312\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.29504281282424927\n",
      "[1,   160] loss: 0.060\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.24075278639793396\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.2711490988731384\n",
      "[1,   180] loss: 0.060\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.12200427800416946\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.14602281153202057\n",
      "[1,   200] loss: 0.064\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.17346130311489105\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.2032475769519806\n",
      "[1,   220] loss: 0.052\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.12234117090702057\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.14548739790916443\n",
      "[1,   240] loss: 0.060\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.2060701698064804\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.2510560154914856\n",
      "[1,   260] loss: 0.059\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.1436019241809845\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.17105549573898315\n",
      "[1,   280] loss: 0.055\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.09667788445949554\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.11298549920320511\n",
      "[1,   300] loss: 0.059\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.028895225375890732\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.03268664330244064\n",
      "[1,   320] loss: 0.064\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.023872729390859604\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.029054854065179825\n",
      "[1,   340] loss: 0.066\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.07043245434761047\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.08278299868106842\n",
      "[1,   360] loss: 0.065\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.03510311245918274\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.023743556812405586\n",
      "[1,   380] loss: 0.055\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.07783730328083038\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.09453783184289932\n",
      "[1,   400] loss: 0.063\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.19367216527462006\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.24272730946540833\n",
      "Parameter containing:\n",
      "tensor([[ 9.9749e-01,  1.3676e-02,  2.5320e-03, -1.2205e-03],\n",
      "        [ 3.7922e-03,  1.0086e+00,  1.4986e-03, -3.7990e-04],\n",
      "        [ 6.2220e-03,  1.3122e-02,  1.0045e+00, -2.9639e-03],\n",
      "        [ 5.1145e-03,  1.5561e-02,  6.5787e-04,  9.9321e-01]],\n",
      "       requires_grad=True)\n",
      "[2,    20] loss: 0.051\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.20685026049613953\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.2608933746814728\n",
      "[2,    40] loss: 0.059\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.04069504141807556\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.05197148397564888\n",
      "[2,    60] loss: 0.064\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.14554624259471893\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.18236981332302094\n",
      "[2,    80] loss: 0.058\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.1546228677034378\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.19481611251831055\n",
      "[2,   100] loss: 0.061\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.20212669670581818\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.27242323756217957\n",
      "[2,   120] loss: 0.058\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.10062140971422195\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.13217522203922272\n",
      "[2,   140] loss: 0.053\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.14969132840633392\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.19804343581199646\n",
      "[2,   160] loss: 0.063\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.05468446761369705\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.0691964328289032\n",
      "[2,   180] loss: 0.049\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.06253646314144135\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.0774427130818367\n",
      "[2,   200] loss: 0.053\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.032761115580797195\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.03497735783457756\n",
      "[2,   220] loss: 0.064\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.1670873761177063\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.21107175946235657\n",
      "[2,   240] loss: 0.070\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.033548902720212936\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.04163328930735588\n",
      "[2,   260] loss: 0.071\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.033628351986408234\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.028289860114455223\n",
      "[2,   280] loss: 0.057\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.2119218111038208\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.267223596572876\n",
      "[2,   300] loss: 0.070\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.29082998633384705\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.35994797945022583\n",
      "[2,   320] loss: 0.058\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.2335643619298935\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.2949231266975403\n",
      "[2,   340] loss: 0.055\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.19286394119262695\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.24349793791770935\n",
      "[2,   360] loss: 0.051\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.052564483135938644\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.06893143057823181\n",
      "[2,   380] loss: 0.062\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.059977635741233826\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.07334338128566742\n",
      "[2,   400] loss: 0.058\n",
      "===========\n",
      "gradient:weight_ih_l0\n",
      "----------\n",
      "0.06157655268907547\n",
      "===========\n",
      "gradient:weight_hh_l0\n",
      "----------\n",
      "0.07353328168392181\n",
      "Parameter containing:\n",
      "tensor([[ 9.9721e-01,  1.3445e-02,  1.3310e-03, -1.7271e-03],\n",
      "        [ 4.1932e-03,  1.0088e+00,  1.2877e-03, -2.6149e-04],\n",
      "        [ 9.6033e-03,  1.4688e-02,  1.0069e+00, -1.2120e-03],\n",
      "        [ 4.9829e-03,  1.5728e-02,  2.0702e-04,  9.9295e-01]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 4**k_size\n",
    "embedding_layer_size = 4\n",
    "hidden_size = 16\n",
    "output_labels=2\n",
    "initial_embedding = get_initial_embeddings(k_size)\n",
    "model = GRU(vocab_size, embedding_layer_size, hidden_size, output_labels,weights_matrix=initial_embedding)\n",
    "model.to(device)\n",
    "val_accuracies = []\n",
    "train_accuracies = []\n",
    "\n",
    "hidden = model.initHidden(batch_size,hidden_size)\n",
    "print(hidden)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02)\n",
    "\n",
    "losses = []\n",
    "running_loss = 0\n",
    "epochs = 2\n",
    "\n",
    "train_loader, validation_loader, test_loader = split_sets(dataset)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for b, (x, y) in enumerate(train_loader):\n",
    "        #gives batches of size \"batch_size, read_length\"\n",
    "        model.zero_grad()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = x.type(torch.LongTensor)\n",
    "        y = y.type(torch.LongTensor)\n",
    "            \n",
    "        out,_,hidden = model(hidden,x)\n",
    "        y = y.view(batch_size*read_length)\n",
    "        out = out.view(batch_size*read_length,output_labels)\n",
    "        loss = loss_function(out,y)\n",
    "        running_loss = running_loss + loss.item()\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if b % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, b + 1, running_loss / 100))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "            check_gradients(model)\n",
    "            \n",
    "        hidden.detach_()\n",
    "    train_acc = accuracy_test(train_loader,model,hidden)\n",
    "    val_acc = accuracy_test(validation_loader,model,hidden)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_accuracies.append(train_acc)\n",
    "    losses.append(running_loss)\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(loader,model,hidden):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        for b, (x, y) in enumerate(loader):\n",
    "            x = x.type(torch.LongTensor)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            out,_,hidden = model(hidden,x)\n",
    "            y = y.view(batch_size*read_length)\n",
    "            out = out.view(batch_size*read_length,output_labels).exp()\n",
    "            out_index = torch.max(out,dim=-1).indices\n",
    "            correct += (out_index.eq(y)).sum()\n",
    "            total += len(y)\n",
    "    return correct.item()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9124969951923076"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test(train_loader,model,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
