{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader.data_loader import PhageLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import seaborn as sns\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, output_labels, number_of_layers=1, bidirectional=True, weights_matrix=None):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim_dense = hidden_size\n",
    "        self.num_layers = number_of_layers \n",
    "        if bidirectional:\n",
    "            self.hidden_dim_dense = hidden_size * 2\n",
    "        if len(weights_matrix.size())!=0:\n",
    "            self.emb_layer = self.create_emb_layer(weights_matrix,True)          \n",
    "        else:\n",
    "            self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "            \n",
    "        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, bidirectional=bidirectional, num_layers=number_of_layers,batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim_dense, int(self.hidden_dim_dense/2))\n",
    "        self.linear2 = nn.Linear(int(self.hidden_dim_dense/2),output_labels)\n",
    "        \n",
    "    def forward(self, h_t1, indexes):\n",
    "        #indexes -> (batch,seq_length)\n",
    "        embedding = self.emb_layer(indexes)\n",
    "        #print(\"EMBEDDING SHAPE: \", embedding.size())\n",
    "        #embedding -> (batch,seq_length,embedding_size)\n",
    "        out,h_t = self.gru(embedding, h_t1)\n",
    "        #print(\"OUT SHAPE: \", out.size(), self.hidden_dim_dense)\n",
    "        #out -> ()\n",
    "        #out = out.view(self.hidden_dim_dense, -1)\n",
    "        \n",
    "        out = F.relu(self.linear(out))\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out,dim=2)\n",
    "        \n",
    "        return out,h_t\n",
    "        \n",
    "    def initHidden(self, batch_size, hidden_size):      \n",
    "        if self.bidirectional:\n",
    "            return torch.randn(self.num_layers*2, batch_size, hidden_size, device=device)\n",
    "        else:\n",
    "            return torch.randn(self.num_layers, batch_size, hidden_size, device=device)\n",
    "\n",
    "\n",
    "    def create_emb_layer(self, weights_matrix, non_trainable=True):\n",
    "        num_embeddings, embedding_dim = weights_matrix.size()\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "        if non_trainable:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PhageLoader(\"data/\")\n",
    "read_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size=1\n",
    "batch_size = 30\n",
    "read_length = 100\n",
    "dataset = loader.get_data_set(n_files='all',read_length=read_length, batch_size=batch_size, k=k_size, stride=1, embedding=\"dict\", embed_size=None, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(dataset,batch_size):\n",
    "    n = len(dataset)  # how many total elements you have\n",
    "    test_size = .1\n",
    "    n_test = int( n * test_size )  # number of test/val elements\n",
    "    n_train = n - 2 * n_test\n",
    "\n",
    "    idx = list(range(n))  # indices to all elements\n",
    "    np.random.shuffle(idx)  # in-place shuffle the indices to facilitate random splitting\n",
    "    train_idx = idx[:n_train]\n",
    "    val_idx = idx[n_train:(n_train + n_test)]\n",
    "    test_idx = idx[(n_train + n_test):]\n",
    "\n",
    "    print(n,len(train_idx),len(val_idx),len(test_idx))\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(val_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    a = batch_size \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=a, \n",
    "                                               sampler=train_sampler,drop_last=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset, batch_size=a,\n",
    "                                                    sampler=valid_sampler,drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=a,\n",
    "                                                    sampler=test_sampler,drop_last=True)\n",
    "    return train_loader, validation_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_embeddings(loader,k=3):\n",
    "    if(k<=2):\n",
    "        return torch.from_numpy(np.eye(4**k))\n",
    "    dictionary = loader.get_dict(k,'dna2vec')\n",
    "    indexes = loader.get_dict(k)\n",
    "    size = len(indexes)\n",
    "    matrix = np.zeros((size,100))\n",
    "    for key, value in indexes.items():\n",
    "        matrix[value] = dictionary[key]\n",
    "    return torch.from_numpy(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(model):\n",
    "    gru = model.gru\n",
    "    for p,n in zip(gru.parameters(),gru._all_weights[0]):\n",
    "        if n[:6] == 'weight':\n",
    "            print('===========\\ngradient:{}\\n----------\\n{}'.format(n,p.grad.abs().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_params(model):\n",
    "    gru = model.gru\n",
    "    for p,n in zip(gru.parameters(),gru._all_weights[0]):\n",
    "        if n[:6] == 'weight':\n",
    "            print('===========\\ngradient:{}\\n----------\\n{}'.format(n,p.data))\n",
    "            print('===========\\ngradient:{}\\n----------\\n{}'.format(n,p.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(loader,model,hidden):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        for b, (x, y) in enumerate(loader):\n",
    "            x = x.type(torch.LongTensor)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out,hidden = model(hidden,x)\n",
    "            y = y.view(batch_size*read_length)\n",
    "            out = out.view(batch_size*read_length,output_labels).exp()\n",
    "            _, out_index= torch.max(out,dim=-1)\n",
    "            #print(out_index)\n",
    "            correct += (out_index.eq(y)).sum()\n",
    "            total += len(y)\n",
    "    return correct.item()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_plot(model,loader,hidden):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        bs = []\n",
    "        outs = []\n",
    "        ys = []\n",
    "        for b, (x, y) in enumerate(loader):\n",
    "            x = x.type(torch.LongTensor)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out, _  = model(hidden,x)\n",
    "            y = y.view(batch_size*read_length)\n",
    "            out = out.view(batch_size*read_length,output_labels).exp()\n",
    "            _, out_index= torch.max(out,dim=-1)\n",
    "            bs.append(np.ones(y.shape[0])*b)\n",
    "            outs.append(out_index.cpu().numpy())\n",
    "            ys.append(y.cpu().numpy())\n",
    "            \n",
    "        print(b)\n",
    "    return pd.DataFrame({\"batch\": np.concatenate(bs), \"predicted\": np.concatenate(outs),\"actual\": np.concatenate(ys)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_strip(dataframe):\n",
    "    df1 = dataframe[dataframe['batch'] == 0]\n",
    "    df = pd.melt(df1, id_vars=['batch'], value_vars=['predicted','actual'])\n",
    "    df['x'] = np.tile(np.arange(len(df1))+1,2)\n",
    "    colors = [\"windows blue\", \"amber\"]\n",
    "    sns.stripplot(x=\"x\", y=\"variable\", data=df,hue='value', linewidth=1,jitter=True,palette=sns.xkcd_palette(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_validation_set(model,hidden, dataloader, batch_size, read_length, loss_function):\n",
    "    running_loss = 0\n",
    "    output_labels = 2\n",
    "    for b, (x, y) in enumerate(dataloader):\n",
    "        x = x.type(torch.LongTensor)\n",
    "        y = y.type(torch.LongTensor)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out,hidden = model(hidden,x)\n",
    "        y = y.view(batch_size*read_length)\n",
    "        out = out.view(batch_size*read_length,output_labels)\n",
    "        valid_loss = loss_function(out,y)\n",
    "        running_loss = running_loss + valid_loss.item()\n",
    "        \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_net(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer,n_files='all',id_run=1):\n",
    "    k_size=k_size\n",
    "    batch_size = batch_size\n",
    "    read_length = read_length\n",
    "    loader = PhageLoader(\"data/\")\n",
    "    dataset = loader.get_data_set(n_files=n_files,read_length=read_length, batch_size=batch_size, k=k_size, stride=stride, embedding=\"dict\", embed_size=None, drop_last=False)\n",
    "    vocab_size = 4**k_size\n",
    "    if k_size <= 2:   \n",
    "        embedding_layer_size = vocab_size\n",
    "    else:\n",
    "        embedding_layer_size = 100\n",
    "        \n",
    "    hidden_size = hidden_size\n",
    "    number_of_layers = number_of_layers\n",
    "    output_labels = 2\n",
    "    initial_embedding = get_initial_embeddings(loader,k_size)\n",
    "    model = GRU(vocab_size, embedding_layer_size, hidden_size, output_labels,number_of_layers=number_of_layers,weights_matrix=initial_embedding)\n",
    "    model.to(device)\n",
    "    weights = torch.tensor([7,1],dtype=torch.float)\n",
    "    weights = weights.to(device)\n",
    "    val_accuracies = []\n",
    "    train_accuracies = []\n",
    "    learning_rate = lr\n",
    "    optim_type = optimizer\n",
    "    \n",
    "    hidden = model.initHidden(batch_size, hidden_size)\n",
    "\n",
    "\n",
    "    loss_function = nn.NLLLoss(weight = weights)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    if optim_type == 'ADAM':\n",
    "        optimizer = optim.Adam(model.parameters(), lr = learning_rate)     \n",
    "\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "    running_loss = 0\n",
    "    \n",
    "    epochs = 100\n",
    "\n",
    "    train_loader, validation_loader, test_loader = split_sets(dataset,batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss_valid = 0\n",
    "        for b, (x, y) in enumerate(train_loader):\n",
    "            #gives batches of size \"batch_size, read_length\"\n",
    "            model.zero_grad()\n",
    "\n",
    "            x = x.type(torch.LongTensor)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            x, y = x.to(device), y.to(device)  \n",
    "            out,hidden = model(hidden,x)\n",
    "            y = y.view(batch_size*read_length)\n",
    "            out = out.view(batch_size*read_length,output_labels)\n",
    "            loss = loss_function(out,y)\n",
    "            running_loss = running_loss + loss.item()\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #if b % 40 == 39:\n",
    "                #print('[%d, %5d] loss: %.3f' %\n",
    "                #      (epoch + 1, b + 1, running_loss / 100))\n",
    "                #losses.append(running_loss)\n",
    "                #running_loss = 0.0\n",
    "                #check_gradients(model)\n",
    "\n",
    "            hidden.detach_()\n",
    "        if (epoch+1) %20 == 0:\n",
    "            print('saving model')\n",
    "            name = 'models_grid/model' + str(id_run)+'_'+str(epoch+1)\n",
    "            torch.save(model,name)\n",
    "            \n",
    "        valid_loss = loss_validation_set(model,hidden,validation_loader,batch_size,read_length,loss_function)  \n",
    "        losses_val.append(valid_loss)\n",
    "        losses.append(running_loss)\n",
    "        running_loss = 0.0\n",
    "        #print(losses,losses_val)\n",
    "    \n",
    "    \n",
    "    filehandler = open('models_grid/losses_train'+str(id_run)+'.pkl',\"wb\")\n",
    "    pickle.dump(losses,filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    filehandler = open('models_grid/losses_validation'+str(id_run)+'.pkl',\"wb\")\n",
    "    pickle.dump(losses_val,filehandler)\n",
    "    filehandler.close()\n",
    "    create_dict(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optim_type,id_run=id_run)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer,id_run=1):\n",
    "    data = {}\n",
    "    data['k_size'] = k_size\n",
    "    data['stride'] = stride\n",
    "    data['batch_size'] = batch_size\n",
    "    data['read_length'] = read_length\n",
    "    data['hidden_size'] = hidden_size\n",
    "    data['number_of_layers'] = number_of_layers\n",
    "    data['lr'] = lr\n",
    "    data['optimizer'] = optimizer\n",
    "    print(data)\n",
    "    name_dict = 'models_grid/model'+ str(id_run)+'.json'\n",
    "    with open(name_dict, 'w') as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer):\n",
    "    g = np.meshgrid(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer)\n",
    "    z = list(zip(*(x.flat for x in g)))\n",
    "    for i,option in enumerate(z): \n",
    "        k_size_p = option[0].item()\n",
    "        stride_p = option[1].item()\n",
    "        batch_size_p =option[2].item()\n",
    "        read_length_p = option[3].item()\n",
    "        hidden_size_p = option[4].item()\n",
    "        number_of_layers_p = option[5].item()\n",
    "        lr_p = option[6].item()\n",
    "        optimizer_p = option[7].item()\n",
    "        print(k_size_p,stride_p,batch_size_p,read_length_p,hidden_size_p,number_of_layers_p,lr_p,optimizer_p)\n",
    "        train_net(k_size_p,stride_p,batch_size_p,read_length_p,hidden_size_p,number_of_layers_p,lr_p,optimizer_p,n_files='all',id_run=i+1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 20 100 30 1 0.07 SGD\n"
     ]
    }
   ],
   "source": [
    "k_size= [3,5,7]\n",
    "stride = [1,2]\n",
    "batch_size = [20,50]\n",
    "read_length = [100,200]\n",
    "hidden_size = [30,60]\n",
    "number_of_layers = [1,2]\n",
    "lr = [0.07, 0.3]\n",
    "optimizer = ['SGD','ADAM']\n",
    "grid_search(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
