{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader.data_loader import PhageLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data import Subset\n",
    "import seaborn as sns\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, output_labels, number_of_layers=1, bidirectional=True, weights_matrix=None):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim_dense = hidden_size\n",
    "        self.num_layers = number_of_layers \n",
    "        if bidirectional:\n",
    "            self.hidden_dim_dense = hidden_size * 2\n",
    "        if len(weights_matrix.size())!=0:\n",
    "            self.emb_layer = self.create_emb_layer(weights_matrix,True)          \n",
    "        else:\n",
    "            self.emb_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "            \n",
    "        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, bidirectional=bidirectional, num_layers=number_of_layers,batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim_dense, int(self.hidden_dim_dense/2))\n",
    "        self.linear2 = nn.Linear(int(self.hidden_dim_dense/2),output_labels)\n",
    "        \n",
    "    def forward(self, h_t1, indexes):\n",
    "        #indexes -> (batch,seq_length)\n",
    "        embedding = self.emb_layer(indexes)\n",
    "        #print(\"EMBEDDING SHAPE: \", embedding.size())\n",
    "        #embedding -> (batch,seq_length,embedding_size)\n",
    "        out,h_t = self.gru(embedding, h_t1)\n",
    "        #print(\"OUT SHAPE: \", out.size(), self.hidden_dim_dense)\n",
    "        #out -> ()\n",
    "        #out = out.view(self.hidden_dim_dense, -1)\n",
    "        \n",
    "        out = F.relu(self.linear(out))\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out,dim=2)\n",
    "        \n",
    "        return out,h_t\n",
    "        \n",
    "    def initHidden(self, batch_size, hidden_size):      \n",
    "        if self.bidirectional:\n",
    "            return torch.randn(self.num_layers*2, batch_size, hidden_size, device=device)\n",
    "        else:\n",
    "            return torch.randn(self.num_layers, batch_size, hidden_size, device=device)\n",
    "\n",
    "\n",
    "    def create_emb_layer(self, weights_matrix, non_trainable=True):\n",
    "        num_embeddings, embedding_dim = weights_matrix.size()\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "        if non_trainable:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_embeddings(loader,k=3):\n",
    "    if(k<=2):\n",
    "        return torch.from_numpy(np.eye(4**k))\n",
    "    dictionary = loader.get_dict(k,'dna2vec')\n",
    "    indexes = loader.get_dict(k)\n",
    "    size = len(indexes)\n",
    "    matrix = np.zeros((size,100))\n",
    "    for key, value in indexes.items():\n",
    "        matrix[value] = dictionary[key]\n",
    "    return torch.from_numpy(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(model):\n",
    "    gru = model.gru\n",
    "    for p,n in zip(gru.parameters(),gru._all_weights[0]):\n",
    "        if n[:6] == 'weight':\n",
    "            print('===========\\ngradient:{}\\n----------\\n{}'.format(n,p.grad.abs().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_params(model):\n",
    "    gru = model.gru\n",
    "    for p,n in zip(gru.parameters(),gru._all_weights[0]):\n",
    "        #if n[:6] == 'weight':\n",
    "            print('===========\\ngradient:{}\\n----------\\n{}'.format(n,p.data))\n",
    "            print('===========\\ngradient:{}\\n----------\\n{}'.format(n,p.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(loader,model,hidden):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        for b, (x, y) in enumerate(loader):\n",
    "            x = x.type(torch.LongTensor)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out,hidden = model(hidden,x)\n",
    "            y = y.view(batch_size*read_length)\n",
    "            out = out.view(batch_size*read_length,output_labels).exp()\n",
    "            _, out_index= torch.max(out,dim=-1)\n",
    "            #print(out_index)\n",
    "            correct += (out_index.eq(y)).sum()\n",
    "            total += len(y)\n",
    "    return correct.item()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_plot(model,loader,hidden):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        bs = []\n",
    "        outs = []\n",
    "        ys = []\n",
    "        for b, (x, y) in enumerate(loader):\n",
    "            x = x.type(torch.LongTensor)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out, _  = model(hidden,x)\n",
    "            y = y.view(batch_size*read_length)\n",
    "            out = out.view(batch_size*read_length,output_labels).exp()\n",
    "            _, out_index= torch.max(out,dim=-1)\n",
    "            bs.append(np.ones(y.shape[0])*b)\n",
    "            outs.append(out_index.cpu().numpy())\n",
    "            ys.append(y.cpu().numpy())\n",
    "            \n",
    "        print(b)\n",
    "    return pd.DataFrame({\"batch\": np.concatenate(bs), \"predicted\": np.concatenate(outs),\"actual\": np.concatenate(ys)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_strip(dataframe):\n",
    "    df1 = dataframe[dataframe['batch'] == 0]\n",
    "    df = pd.melt(df1, id_vars=['batch'], value_vars=['predicted','actual'])\n",
    "    df['x'] = np.tile(np.arange(len(df1))+1,2)\n",
    "    colors = [\"windows blue\", \"amber\"]\n",
    "    sns.stripplot(x=\"x\", y=\"variable\", data=df,hue='value', linewidth=1,jitter=True,palette=sns.xkcd_palette(colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_validation_set(model,hidden, dataloader, batch_size, read_length, loss_function):\n",
    "    running_loss = 0\n",
    "    output_labels = 2\n",
    "    for b, (x, y) in enumerate(dataloader):\n",
    "        x = x.type(torch.LongTensor)\n",
    "        y = y.type(torch.LongTensor)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out,hidden = model(hidden,x)\n",
    "        y = y.view(batch_size*read_length)\n",
    "        out = out.view(batch_size*read_length,output_labels)\n",
    "        valid_loss = loss_function(out,y)\n",
    "        running_loss = running_loss + valid_loss.item()\n",
    "        \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_net(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer,n_files='all',id_run=1):\n",
    "    k_size=k_size\n",
    "    batch_size = batch_size\n",
    "    read_length = read_length\n",
    "    loader = PhageLoader(\"data/\")\n",
    "    dataset = loader.get_data_set(n_files=n_files,read_length=read_length, batch_size=batch_size, k=k_size, stride=stride, embedding=\"dict\", embed_size=None, drop_last=False)\n",
    "    vocab_size = 4**k_size\n",
    "    if k_size <= 2:   \n",
    "        embedding_layer_size = vocab_size\n",
    "    else:\n",
    "        embedding_layer_size = 100\n",
    "        \n",
    "    hidden_size = hidden_size\n",
    "    number_of_layers = number_of_layers\n",
    "    output_labels = 2\n",
    "    initial_embedding = get_initial_embeddings(loader,k_size)\n",
    "    model = GRU(vocab_size, embedding_layer_size, hidden_size, output_labels,number_of_layers=number_of_layers,weights_matrix=initial_embedding)\n",
    "    model.to(device)\n",
    "    weights = torch.tensor([7,1],dtype=torch.float)\n",
    "    weights = weights.to(device)\n",
    "    val_accuracies = []\n",
    "    train_accuracies = []\n",
    "    learning_rate = lr\n",
    "    optim_type = optimizer\n",
    "    \n",
    "    hidden = model.initHidden(batch_size, hidden_size)\n",
    "\n",
    "\n",
    "    loss_function = nn.NLLLoss(weight = weights)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    if optim_type == 'ADAM':\n",
    "        optimizer = optim.Adam(model.parameters(), lr = learning_rate)     \n",
    "\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "    running_loss = 0\n",
    "    \n",
    "    epochs = 100\n",
    "\n",
    "    train_loader, validation_loader, test_loader = split_sets(dataset,batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss_valid = 0\n",
    "        for b, (x, y) in enumerate(train_loader):\n",
    "            #gives batches of size \"batch_size, read_length\"\n",
    "            model.zero_grad()\n",
    "\n",
    "            x = x.type(torch.LongTensor)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            x, y = x.to(device), y.to(device)  \n",
    "            out,hidden = model(hidden,x)\n",
    "            y = y.view(batch_size*read_length)\n",
    "            out = out.view(batch_size*read_length,output_labels)\n",
    "            loss = loss_function(out,y)\n",
    "            running_loss = running_loss + loss.item()\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #if b % 40 == 39:\n",
    "                #print('[%d, %5d] loss: %.3f' %\n",
    "                #      (epoch + 1, b + 1, running_loss / 100))\n",
    "                #losses.append(running_loss)\n",
    "                #running_loss = 0.0\n",
    "                #check_gradients(model)\n",
    "\n",
    "            hidden.detach_()\n",
    "        if (epoch+1) %20 == 0:\n",
    "            print('saving model')\n",
    "            name = 'models_grid/model' + str(id_run)+'_'+str(epoch+1)\n",
    "            torch.save(model,name)\n",
    "            \n",
    "        valid_loss = loss_validation_set(model,hidden,validation_loader,batch_size,read_length,loss_function)  \n",
    "        losses_val.append(valid_loss)\n",
    "        losses.append(running_loss)\n",
    "        running_loss = 0.0\n",
    "        #print(losses,losses_val)\n",
    "    \n",
    "    \n",
    "    filehandler = open('models_grid/losses_train'+str(id_run)+'.pkl',\"wb\")\n",
    "    pickle.dump(losses,filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "    filehandler = open('models_grid/losses_validation'+str(id_run)+'.pkl',\"wb\")\n",
    "    pickle.dump(losses_val,filehandler)\n",
    "    filehandler.close()\n",
    "    create_dict(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optim_type,id_run=id_run)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer,id_run=1):\n",
    "    data = {}\n",
    "    data['k_size'] = k_size\n",
    "    data['stride'] = stride\n",
    "    data['batch_size'] = batch_size\n",
    "    data['read_length'] = read_length\n",
    "    data['hidden_size'] = hidden_size\n",
    "    data['number_of_layers'] = number_of_layers\n",
    "    data['lr'] = lr\n",
    "    data['optimizer'] = optimizer\n",
    "    print(data)\n",
    "    name_dict = 'models_grid/model'+ str(id_run)+'.json'\n",
    "    with open(name_dict, 'w') as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer):\n",
    "    g = np.meshgrid(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer)\n",
    "    z = list(zip(*(x.flat for x in g)))\n",
    "    for i,option in enumerate(z): \n",
    "        k_size_p = option[0].item()\n",
    "        stride_p = option[1].item()\n",
    "        batch_size_p =option[2].item()\n",
    "        read_length_p = option[3].item()\n",
    "        hidden_size_p = option[4].item()\n",
    "        number_of_layers_p = option[5].item()\n",
    "        lr_p = option[6].item()\n",
    "        optimizer_p = option[7].item()\n",
    "        print(k_size_p,stride_p,batch_size_p,read_length_p,hidden_size_p,number_of_layers_p,lr_p,optimizer_p)\n",
    "        train_net(k_size_p,stride_p,batch_size_p,read_length_p,hidden_size_p,number_of_layers_p,lr_p,optimizer_p,n_files='all',id_run=i+1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 20 100 30 1 0.07 SGD\n"
     ]
    }
   ],
   "source": [
    "k_size= [3,5,7]\n",
    "stride = [1,2]\n",
    "batch_size = [20,50]\n",
    "read_length = [100,200]\n",
    "hidden_size = [30,60]\n",
    "number_of_layers = [1,2]\n",
    "lr = [0.07, 0.3]\n",
    "optimizer = ['SGD','ADAM']\n",
    "grid_search(k_size,stride,batch_size,read_length,hidden_size,number_of_layers,lr,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(dataset,batch_size,ids=False):\n",
    "    n = len(dataset)  # how many total elements you have\n",
    "    test_size = .1\n",
    "    n_test = int( n * test_size )  # number of test/val elements\n",
    "    n_train = n - 2 * n_test\n",
    "\n",
    "    idx = list(range(n))  # indices to all elements\n",
    "    train_idx = idx[:n_train]\n",
    "    val_idx = idx[n_train:(n_train + n_test)]\n",
    "    test_idx = idx[(n_train + n_test):]\n",
    "\n",
    "    train_dataset = Subset(dataset,train_idx)\n",
    "    valid_dataset = Subset(dataset,val_idx)\n",
    "    test_dataset = Subset(dataset,test_idx)\n",
    "    a = batch_size \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=a, drop_last=True)\n",
    "    validation_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=a,drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=a, drop_last=True)\n",
    "    if ids:\n",
    "        return (train_loader,train_idx), (validation_loader,val_idx), (test_loader,test_idx)\n",
    "    else:\n",
    "        return train_loader, validation_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_formatted(model_name='model5',epoch=100):\n",
    "    folder = 'models_grid/'\n",
    "    \n",
    "    model_selected = model_name+'_'+str(epoch)\n",
    "    model = torch.load(folder + model_selected,map_location='cpu')\n",
    "    loader = PhageLoader(\"data/\")\n",
    "    \n",
    "    configuration = model_name+'.json'\n",
    "    f = open(folder + configuration)\n",
    "    config = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    k_size = config['k_size']\n",
    "    read_length = config['read_length']\n",
    "    batch_size = config['batch_size']\n",
    "    stride = config['stride']\n",
    "    hidden_size = config['hidden_size']\n",
    "    batch_size = 1\n",
    "\n",
    "    dataset = loader.get_data_set(n_files=2,read_length=read_length, batch_size=batch_size, k=k_size, stride=stride, embedding=\"dict\", embed_size=None, drop_last=False)\n",
    "    list_ids = loader.get_data_set_ids(n_files=2,read_length=read_length, batch_size=batch_size, k=k_size, stride=stride, embedding=\"dict\", embed_size=None, drop_last=False)\n",
    "\n",
    "    (train_loader,train_idx), (validation_loader,val_idx), (test_loader,test_idx) = split_sets(dataset,batch_size,ids=True)\n",
    "    indices_validation_set = [list_ids[i] for i in val_idx]\n",
    "    dataframe_result = predictions_validation(validation_loader, model, indices_validation_set, batch_size,hidden_size,k_size,read_length)\n",
    "    return dataframe_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_validation(validation_loader,model,indices,batch_size,hidden_size,k_size,read_length):\n",
    "    output_labels = 2\n",
    "    df = pd.DataFrame(columns=['id', 'predicted_values', 'true_values'])\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        hidden = model.initHidden(batch_size,hidden_size)\n",
    "        id_read = None\n",
    "        prediction_batch = []\n",
    "        ytrue_batch = []\n",
    "        init = True\n",
    "        for b, (x, y) in enumerate(validation_loader):\n",
    "            x = x.type(torch.LongTensor)\n",
    "            y = y.type(torch.LongTensor)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out,hidden = model(hidden,x)\n",
    "            \n",
    "            y = y.view(-1, 1).repeat(1, k_size).view(batch_size,-1)\n",
    "            y = get_original(y,k_size,1)\n",
    "            out = out.view(read_length,batch_size,output_labels).exp()\n",
    "            _, out_index= torch.max(out,dim=-1)\n",
    "            out_index = out_index.view(-1, 1).repeat(1, k_size).view(batch_size,-1)\n",
    "            \n",
    "            out_index = get_original(out_index,k_size,1)\n",
    "            \n",
    "            if id_read == indices[b*batch_size: (b+1)*batch_size][0]:\n",
    "                \n",
    "                prediction_batch = prediction_batch + out_index\n",
    "                ytrue_batch = ytrue_batch + y\n",
    "                \n",
    "            else:\n",
    "                if not init:\n",
    "                    df = df.append({'id' : id_read , 'predicted_values' : prediction_batch,'true_values':ytrue_batch} , ignore_index=True)\n",
    "                prediction_batch = out_index\n",
    "                ytrue_batch = y\n",
    "                init = False\n",
    "            id_read = indices[b*batch_size: (b+1)*batch_size][0]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original(predictions,k_size=3,stride=1):\n",
    "    i=0\n",
    "    result = []\n",
    "    while (i+k_size-1) < predictions.size()[1]:\n",
    "        preds = predictions[0,i*stride:(i+k_size)*(stride)]\n",
    "        preds = preds.tolist() \n",
    "        if i==0:\n",
    "            result = result + preds\n",
    "        else:\n",
    "            result.append(preds[k_size-1])\n",
    "        i = i + k_size \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_result = get_results_formatted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted_values</th>\n",
       "      <th>true_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_016073-69</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_016073-70</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_016073-71</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_016073-72</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_016073-73</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NC_016073-74</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NC_016073-76</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NC_016073-77</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NC_016073-78</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NC_016073-79</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NC_016073-81</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NC_016073-82</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NC_016073-83</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NC_016073-85</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NC_016073-86</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NC_016073-87</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                   predicted_values  \\\n",
       "0   NC_016073-69  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1   NC_016073-70  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2   NC_016073-71  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3   NC_016073-72  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4   NC_016073-73  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "5   NC_016073-74  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6   NC_016073-76  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7   NC_016073-77  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8   NC_016073-78  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9   NC_016073-79  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "10  NC_016073-81  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "11  NC_016073-82  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "12  NC_016073-83  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "13  NC_016073-85  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14  NC_016073-86  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "15  NC_016073-87  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                          true_values  \n",
       "0   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "5   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "7   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "8   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...  \n",
       "9   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "10  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "11  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "12  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "13  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "14  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "15  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
